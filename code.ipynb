{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d8cf2d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'catboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mxgboost\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m XGBRegressor\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlightgbm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LGBMRegressor\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcatboost\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CatBoostRegressor\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# ============================================================\u001b[39;00m\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# ADJUSTED RÂ² FUNCTION\u001b[39;00m\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# ============================================================\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34madjusted_r2\u001b[39m(r2, n, p):\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'catboost'"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# MOVIE REVENUE PREDICTION - FULL EXPERIMENT PIPELINE\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# ============================================================\n",
    "# MODELS (FULL LIST)\n",
    "# ============================================================\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestRegressor, AdaBoostRegressor,\n",
    "    GradientBoostingRegressor, BaggingRegressor\n",
    ")\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# ADJUSTED RÂ² FUNCTION\n",
    "# ============================================================\n",
    "\n",
    "def adjusted_r2(r2, n, p):\n",
    "    return 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# MODEL LIST\n",
    "# ============================================================\n",
    "\n",
    "models = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Ridge\": Ridge(),\n",
    "    \"Lasso\": Lasso(),\n",
    "    \"ElasticNet\": ElasticNet(),\n",
    "\n",
    "    \"Decision Tree\": DecisionTreeRegressor(),\n",
    "    \"Random Forest\": RandomForestRegressor(),\n",
    "    \"Ada Boost\": AdaBoostRegressor(),\n",
    "    \"Gradient Boost\": GradientBoostingRegressor(),\n",
    "\n",
    "    \"LGBM\": LGBMRegressor(),\n",
    "    \"XGBoost\": XGBRegressor(),\n",
    "   \n",
    "\n",
    "    \"SVM\": SVR(),\n",
    "    \"KNN\": KNeighborsRegressor(),\n",
    "    \"Bagging Regressor\": BaggingRegressor()\n",
    "}\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# EVALUATION FUNCTION (ALL MODELS)\n",
    "# ============================================================\n",
    "\n",
    "def evaluate_all_models(stage_name, X_train, X_test, y_train, y_test):\n",
    "\n",
    "    print(f\"\\n\\n==============================\")\n",
    "    print(f\"ðŸ“Œ STAGE: {stage_name}\")\n",
    "    print(f\"==============================\")\n",
    "\n",
    "    results = []\n",
    "    p = X_train.shape[1]\n",
    "\n",
    "    for name, model in models.items():\n",
    "\n",
    "        pipe = Pipeline([\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"model\", model)\n",
    "        ])\n",
    "\n",
    "        pipe.fit(X_train, y_train)\n",
    "\n",
    "        train_pred = pipe.predict(X_train)\n",
    "        test_pred = pipe.predict(X_test)\n",
    "\n",
    "        train_r2 = r2_score(y_train, train_pred)\n",
    "        test_r2 = r2_score(y_test, test_pred)\n",
    "\n",
    "        train_adj = adjusted_r2(train_r2, len(y_train), p)\n",
    "        test_adj = adjusted_r2(test_r2, len(y_test), p)\n",
    "\n",
    "        test_rmse = np.sqrt(mean_squared_error(y_test, test_pred))\n",
    "\n",
    "        gap = abs(train_adj - test_adj)\n",
    "\n",
    "        results.append([name, train_adj, test_adj, test_rmse, gap])\n",
    "\n",
    "    df = pd.DataFrame(results, columns=[\n",
    "        \"Model\", \"Train Adj R2\", \"Test Adj R2\", \"RMSE\", \"Overfit Gap\"\n",
    "    ])\n",
    "\n",
    "    df = df.sort_values(by=[\"Test Adj R2\", \"Overfit Gap\"], ascending=[False, True])\n",
    "\n",
    "    print(\"\\nTop 5 Models:\")\n",
    "    print(df.head(5))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# STEP 1: LOAD DATA\n",
    "# ============================================================\n",
    "\n",
    "data = pd.read_csv(\"bollywood_movie_data.csv\")\n",
    "\n",
    "X = data.drop(\"Revenue(INR)\", axis=1)\n",
    "y = data[\"Revenue(INR)\"]\n",
    "\n",
    "print(\"\\nDataset Loaded:\", data.shape)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# STAGE 1: BASELINE (OneHot Encoding)\n",
    "# ============================================================\n",
    "\n",
    "X_base = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_base, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "baseline_results = evaluate_all_models(\n",
    "    \"Baseline (OneHot Encoding Only)\",\n",
    "    X_train, X_test, y_train, y_test\n",
    ")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# STAGE 2: FREQUENCY ENCODING HIGH CARDINALITY\n",
    "# ============================================================\n",
    "\n",
    "X_freq = X.copy()\n",
    "\n",
    "high_card_cols = [\n",
    "    \"Movie Name\", \"Lead Star\", \"Director\",\n",
    "    \"Music Director\", \"New Actor\",\n",
    "    \"New Director\", \"New Music Director\"\n",
    "]\n",
    "\n",
    "for col in high_card_cols:\n",
    "    if col in X_freq.columns:\n",
    "        freq_map = X_freq[col].value_counts().to_dict()\n",
    "        X_freq[col] = X_freq[col].map(freq_map)\n",
    "\n",
    "X_freq = pd.get_dummies(X_freq, drop_first=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_freq, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "freq_results = evaluate_all_models(\n",
    "    \"After Frequency Encoding\",\n",
    "    X_train, X_test, y_train, y_test\n",
    ")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# STAGE 3: LOG TARGET TRANSFORMATION\n",
    "# ============================================================\n",
    "\n",
    "y_log = np.log1p(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_freq, y_log, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "log_results = evaluate_all_models(\n",
    "    \"After Log Target Transform\",\n",
    "    X_train, X_test, y_train, y_test\n",
    ")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# STAGE 4: HYPERPARAMETER TUNING (Best Model Only)\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\\n==============================\")\n",
    "print(\"ðŸ“Œ STAGE: Hyperparameter Tuning\")\n",
    "print(\"==============================\")\n",
    "\n",
    "best_model_name = log_results.iloc[0][\"Model\"]\n",
    "print(\"Best Model Before Tuning:\", best_model_name)\n",
    "\n",
    "best_model = models[best_model_name]\n",
    "\n",
    "# Example tuning for RandomForest / XGB / LGBM only\n",
    "param_grid = {}\n",
    "\n",
    "if best_model_name == \"Random Forest\":\n",
    "    param_grid = {\n",
    "        \"model__n_estimators\": [100, 200],\n",
    "        \"model__max_depth\": [5, 10, None]\n",
    "    }\n",
    "\n",
    "elif best_model_name == \"XGBoost\":\n",
    "    param_grid = {\n",
    "        \"model__n_estimators\": [100, 200],\n",
    "        \"model__learning_rate\": [0.05, 0.1]\n",
    "    }\n",
    "\n",
    "elif best_model_name == \"LGBM\":\n",
    "    param_grid = {\n",
    "        \"model__n_estimators\": [100, 200],\n",
    "        \"model__learning_rate\": [0.05, 0.1]\n",
    "    }\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"model\", best_model)\n",
    "])\n",
    "\n",
    "if param_grid:\n",
    "    grid = GridSearchCV(pipe, param_grid, cv=3, scoring=\"r2\", n_jobs=-1)\n",
    "    grid.fit(X_train, y_train)\n",
    "\n",
    "    final_pipeline = grid.best_estimator_\n",
    "    print(\"Best Params:\", grid.best_params_)\n",
    "\n",
    "else:\n",
    "    final_pipeline = pipe.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# FINAL EVALUATION\n",
    "# ============================================================\n",
    "\n",
    "test_pred = final_pipeline.predict(X_test)\n",
    "\n",
    "final_rmse = np.sqrt(mean_squared_error(y_test, test_pred))\n",
    "final_r2 = r2_score(y_test, test_pred)\n",
    "\n",
    "print(\"\\nâœ… FINAL MODEL PERFORMANCE\")\n",
    "print(\"Final RMSE:\", final_rmse)\n",
    "print(\"Final RÂ²:\", final_r2)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# SAVE FINAL MODEL + COLUMNS\n",
    "# ============================================================\n",
    "\n",
    "joblib.dump(final_pipeline, \"best_movie_model.pkl\")\n",
    "joblib.dump(X_freq.columns.tolist(), \"training_columns.pkl\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ FINAL MODEL SAVED!\")\n",
    "print(\"Files:\")\n",
    "print(\"âœ… best_movie_model.pkl\")\n",
    "print(\"âœ… training_columns.pkl\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

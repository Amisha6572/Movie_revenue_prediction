{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f3faa88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Training finished. Files saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import joblib\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "# from sklearn.pipeline import Pipeline\n",
    "\n",
    "# from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# # Models\n",
    "# from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "# from sklearn.tree import DecisionTreeRegressor\n",
    "# from sklearn.ensemble import (\n",
    "#     RandomForestRegressor, AdaBoostRegressor,\n",
    "#     GradientBoostingRegressor, BaggingRegressor\n",
    "# )\n",
    "# from sklearn.svm import SVR\n",
    "# from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# from xgboost import XGBRegressor\n",
    "# from lightgbm import LGBMRegressor\n",
    "\n",
    "\n",
    "\n",
    "# # ---------------------------------------------------\n",
    "# # Adjusted R² Function\n",
    "# # ---------------------------------------------------\n",
    "# def adjusted_r2(r2, n, p):\n",
    "#     return 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
    "\n",
    "\n",
    "# # ---------------------------------------------------\n",
    "# # Load Dataset\n",
    "# # ---------------------------------------------------\n",
    "# df = pd.read_csv(\"bollywood_movie_data.csv\")\n",
    "\n",
    "# print(\"Dataset Shape:\", df.shape)\n",
    "\n",
    "# df.drop(\"Movie Name\", axis=1, inplace=True)\n",
    "# df.drop(\"Release Period\", axis=1, inplace=True)\n",
    "# df.drop(\"New Director\", axis=1, inplace=True)\n",
    "# df.drop(\"Whether Franchise\", axis=1, inplace=True)\n",
    "# TARGET = \"Revenue(INR)\"\n",
    "# y = df[TARGET]\n",
    "# X = df.drop(TARGET, axis=1)\n",
    "\n",
    "\n",
    "# # ---------------------------------------------------\n",
    "# # Identify Columns\n",
    "# # ---------------------------------------------------\n",
    "# cat_cols = X.select_dtypes(include=\"object\").columns.tolist()\n",
    "# num_cols = X.select_dtypes(exclude=\"object\").columns.tolist()\n",
    "\n",
    "# print(\"Categorical Columns:\", len(cat_cols))\n",
    "# print(\"Numeric Columns:\", len(num_cols))\n",
    "\n",
    "\n",
    "# # ---------------------------------------------------\n",
    "# # Reduce High Cardinality Columns\n",
    "# # ---------------------------------------------------\n",
    "# top_categories = {}\n",
    "# def reduce_cardinality(df, col, top_k=15):\n",
    "#     top = df[col].value_counts().nlargest(top_k).index.tolist()\n",
    "#     top_categories[col] = top\n",
    "#     df[col] = df[col].apply(lambda x: x if x in top else \"Other\")\n",
    "#     return df\n",
    "\n",
    "\n",
    "# high_card_cols = [\"Movie Name\", \"Lead Star\", \"Director\", \"Music Director\"]\n",
    "\n",
    "# for col in high_card_cols:\n",
    "#     if col in X.columns:\n",
    "#         X = reduce_cardinality(X, col)\n",
    "\n",
    "\n",
    "# # ---------------------------------------------------\n",
    "# # Train-Test Split\n",
    "# # ---------------------------------------------------\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     X, y, test_size=0.2, random_state=42\n",
    "# )\n",
    "\n",
    "\n",
    "# # ---------------------------------------------------\n",
    "# # Models List (User Provided)\n",
    "# # ---------------------------------------------------\n",
    "# models = {\n",
    "#     \"Linear Regression\": LinearRegression(),\n",
    "#     \"Ridge\": Ridge(),\n",
    "#     \"Lasso\": Lasso(),\n",
    "#     \"ElasticNet\": ElasticNet(),\n",
    "#     \"Decision Tree\": DecisionTreeRegressor(),\n",
    "#     \"Random Forest\": RandomForestRegressor(),\n",
    "#     \"AdaBoost\": AdaBoostRegressor(),\n",
    "#     \"Gradient Boosting\": GradientBoostingRegressor(),\n",
    "#     \"LGBM\": LGBMRegressor(),\n",
    "#     \"XGBoost\": XGBRegressor(),\n",
    "#     \"SVM\": SVR(),\n",
    "#     \"KNN\": KNeighborsRegressor(),\n",
    "#     \"Bagging Regressor\": BaggingRegressor()\n",
    "# }\n",
    "\n",
    "\n",
    "# # ---------------------------------------------------\n",
    "# # Transformation Stages\n",
    "# # ---------------------------------------------------\n",
    "# stages = {\n",
    "#     \"Stage1_OneHotOnly\": ColumnTransformer([\n",
    "#         (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols)\n",
    "#     ], remainder=\"passthrough\"),\n",
    "\n",
    "#     \"Stage2_Scaling+OneHot\": ColumnTransformer([\n",
    "#         (\"num\", StandardScaler(), num_cols),\n",
    "#         (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols)\n",
    "#     ])\n",
    "# }\n",
    "\n",
    "\n",
    "# # ---------------------------------------------------\n",
    "# # Evaluate Models at Each Stage\n",
    "# # ---------------------------------------------------\n",
    "# best_pipeline = None\n",
    "# best_score = -999\n",
    "# best_name = \"\"\n",
    "\n",
    "# print(\"\\n======================================================\")\n",
    "# print(\"MODEL EVALUATION WITH TRAIN + TEST ADJUSTED R²\")\n",
    "# print(\"======================================================\")\n",
    "\n",
    "# for stage_name, transformer in stages.items():\n",
    "\n",
    "#     print(\"\\n======================================================\")\n",
    "#     print(\"Running Transformation Stage:\", stage_name)\n",
    "#     print(\"======================================================\")\n",
    "\n",
    "#     for model_name, model in models.items():\n",
    "\n",
    "#         pipeline = Pipeline([\n",
    "#             (\"transform\", transformer),\n",
    "#             (\"model\", model)\n",
    "#         ])\n",
    "\n",
    "#         pipeline.fit(X_train, y_train)\n",
    "\n",
    "#         # Predictions\n",
    "#         train_pred = pipeline.predict(X_train)\n",
    "#         test_pred = pipeline.predict(X_test)\n",
    "\n",
    "#         # RMSE\n",
    "#         rmse_train = np.sqrt(mean_squared_error(y_train, train_pred))\n",
    "#         rmse_test = np.sqrt(mean_squared_error(y_test, test_pred))\n",
    "\n",
    "#         # R²\n",
    "#         r2_train = r2_score(y_train, train_pred)\n",
    "#         r2_test = r2_score(y_test, test_pred)\n",
    "\n",
    "#         # Adjusted R²\n",
    "#         adj_train = adjusted_r2(r2_train, X_train.shape[0], X_train.shape[1])\n",
    "#         adj_test = adjusted_r2(r2_test, X_test.shape[0], X_test.shape[1])\n",
    "\n",
    "#         # Overfitting Gap\n",
    "#         overfit_gap = adj_train - adj_test\n",
    "\n",
    "#         # Print Results\n",
    "#         print(f\"\"\"\n",
    "# Model: {model_name}\n",
    "# --------------------------------------------------\n",
    "# Train RMSE      : {rmse_train:.2f}\n",
    "# Test RMSE       : {rmse_test:.2f}\n",
    "\n",
    "# Train Adj R²    : {adj_train:.4f}\n",
    "# Test Adj R²     : {adj_test:.4f}\n",
    "\n",
    "# Overfit Gap     : {overfit_gap:.4f}\n",
    "# --------------------------------------------------\n",
    "#         \"\"\")\n",
    "\n",
    "#         # Best Tradeoff Score\n",
    "#         score = adj_test - (0.3 * overfit_gap)\n",
    "\n",
    "#         if score > best_score:\n",
    "#             best_score = score\n",
    "#             best_pipeline = pipeline\n",
    "#             best_name = f\"{stage_name} + {model_name}\"\n",
    "\n",
    "\n",
    "# # ---------------------------------------------------\n",
    "# # Best Model Selected\n",
    "# # ---------------------------------------------------\n",
    "# print(\"\\n======================================================\")\n",
    "# print(\"✅ BEST MODEL SELECTED\")\n",
    "# print(\"======================================================\")\n",
    "# print(\"Best Model:\", best_name)\n",
    "\n",
    "# # Save Final Pipeline\n",
    "# joblib.dump(best_pipeline, \"movie_pipeline.pkl\")\n",
    "# print(\"\\n✅ Saved Successfully: movie_pipeline.pkl\")\n",
    "\n",
    "# joblib.dump(X.columns.tolist(), \"training_columns.pkl\")\n",
    "# print(\"✅ training_columns.pkl saved\")\n",
    "\n",
    "# joblib.dump(cat_cols, \"categorical_cols.pkl\")\n",
    "# joblib.dump(num_cols, \"numeric_cols.pkl\")\n",
    "# joblib.dump(top_categories, \"top_categories.pkl\")\n",
    "# print(\"✅ Saved: top_categories.pkl\")\n",
    "# print(\"✅ Saved: categorical_cols.pkl, numeric_cols.pkl\")   \n",
    "\n",
    "#---------------------------------------------------\n",
    "# second version with better structure and comments for clarity\n",
    "#---------------------------------------------------\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import joblib\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.ensemble import RandomForestRegressor # Example best model\n",
    "\n",
    "# # Load Dataset\n",
    "# df = pd.read_csv(\"bollywood_movie_data.csv\")\n",
    "\n",
    "# # 1. Drop unnecessary columns immediately\n",
    "# cols_to_drop = [\"Movie Name\", \"Release Period\", \"New Director\", \"Whether Franchise\"]\n",
    "# df.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "# TARGET = \"Revenue(INR)\"\n",
    "# # Use Log Transformation for better scaling (optional but recommended for money)\n",
    "# y = np.log1p(df[TARGET]) \n",
    "# X = df.drop(TARGET, axis=1)\n",
    "\n",
    "# # 2. Identify Columns AFTER dropping\n",
    "# cat_cols = X.select_dtypes(include=\"object\").columns.tolist()\n",
    "# num_cols = X.select_dtypes(exclude=\"object\").columns.tolist()\n",
    "\n",
    "# # 3. Handle High Cardinality\n",
    "# top_categories = {}\n",
    "# def reduce_cardinality(df, col, top_k=15):\n",
    "#     top = df[col].value_counts().nlargest(top_k).index.tolist()\n",
    "#     top_categories[col] = top\n",
    "#     df[col] = df[col].apply(lambda x: x if x in top else \"Other\")\n",
    "#     return df\n",
    "\n",
    "# high_card_cols = [\"Lead Star\", \"Director\", \"Music Director\"]\n",
    "# for col in high_card_cols:\n",
    "#     if col in X.columns:\n",
    "#         X = reduce_cardinality(X, col)\n",
    "\n",
    "# # 4. Define Pipeline\n",
    "# preprocessor = ColumnTransformer([\n",
    "#     (\"num\", StandardScaler(), num_cols),\n",
    "#     (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols)\n",
    "# ])\n",
    "\n",
    "# # Using RandomForest as an example - replace with your best found model\n",
    "# best_pipeline = Pipeline([\n",
    "#     (\"transform\", preprocessor),\n",
    "#     (\"model\", RandomForestRegressor(n_estimators=100, random_state=42))\n",
    "# ])\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# best_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# # 5. Save everything consistently\n",
    "# joblib.dump(best_pipeline, \"movie_pipeline.pkl\")\n",
    "# joblib.dump(cat_cols, \"categorical_cols.pkl\")\n",
    "# joblib.dump(num_cols, \"numeric_cols.pkl\")\n",
    "# joblib.dump(top_categories, \"top_categories.pkl\")\n",
    "\n",
    "# print(\"✅ Training Complete and Files Saved!\")\n",
    "\n",
    "#---------------------------------------------------\n",
    "#third version with modular functions for better readability and maintenance\n",
    "#---------------------------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Load Dataset\n",
    "df = pd.read_csv(\"bollywood_movie_data.csv\")\n",
    "\n",
    "# 1. Drop the columns you don't want in the app\n",
    "cols_to_drop = [\"Movie Name\", \"Release Period\", \"New Director\", \"Whether Franchise\"]\n",
    "df.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "TARGET = \"Revenue(INR)\"\n",
    "# Use log transformation because Revenue has high variance\n",
    "y = np.log1p(df[TARGET]) \n",
    "X = df.drop(TARGET, axis=1)\n",
    "\n",
    "# 2. Identify Columns AFTER dropping\n",
    "cat_cols = X.select_dtypes(include=\"object\").columns.tolist()\n",
    "num_cols = X.select_dtypes(exclude=\"object\").columns.tolist()\n",
    "\n",
    "# 3. Handle High Cardinality AND save Genre options\n",
    "top_categories = {}\n",
    "def reduce_cardinality(df, col, top_k=15):\n",
    "    # Get the top categories\n",
    "    top = df[col].value_counts().nlargest(top_k).index.tolist()\n",
    "    top_categories[col] = top\n",
    "    # Replace anything else with 'Other'\n",
    "    df[col] = df[col].apply(lambda x: x if x in top else \"Other\")\n",
    "    return df\n",
    "\n",
    "# Apply this to High Cardinality columns\n",
    "for col in [\"Lead Star\", \"Director\", \"Music Director\"]:\n",
    "    X = reduce_cardinality(X, col)\n",
    "\n",
    "# Save ALL unique values for Genre (since it's not high cardinality, we keep all)\n",
    "top_categories[\"Genre\"] = X[\"Genre\"].unique().tolist()\n",
    "\n",
    "# 4. Define and Train Pipeline\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", StandardScaler(), num_cols),\n",
    "    (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols)\n",
    "])\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"transform\", preprocessor),\n",
    "    (\"model\", RandomForestRegressor(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# 5. Save everything\n",
    "joblib.dump(pipeline, \"movie_pipeline.pkl\")\n",
    "joblib.dump(cat_cols, \"categorical_cols.pkl\")\n",
    "joblib.dump(num_cols, \"numeric_cols.pkl\")\n",
    "joblib.dump(top_categories, \"top_categories.pkl\")\n",
    "\n",
    "print(\"✅ Training finished. Files saved successfully.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
